experiment:
  dataset: ncf_pinterest
  data_config:
    strategy: fixed
    train_path: ../data/{0}/pinterest-20.train.rating
    test_path: ../data/{0}/pinterest-20.test.rating
  binarize: True
  negative_sampling:
    strategy: fixed
    files: [ "/home/ironman/PycharmProjects/Elliot/data/ncf_pinterest/pinterest-20.test.negative" ]
  top_k: 10
  evaluation:
    cutoffs: 10
    simple_metrics: [HR]
  gpu: 0
  external_models_path: ../external/models/__init__.py
  models:
#    Random:
#      meta:
#        save_recs: True
#    external.MostPop:
#      meta:
#        verbose: True
#        save_recs: True
#    external.RendleMF: # from original paper
#      meta:
#        hyper_max_evals: 1
#        hyper_opt_alg: tpe
#        validation_rate: 1
#        verbose: True
#        save_recs: True
##        optimize_internal_loss: True
#      epochs: 256 # 256 original paper but 50 comes from NeuMF paper
#      factors: 32
#      lr: 0.007
#      reg: 0.01
#      m: 10
#      random_seed: 42
#    external.NeuMF: #from the original paper + Rendle
#      meta:
#        hyper_max_evals: 1
#        hyper_opt_alg: tpe
#        verbose: True
#        save_recs: True
#        validation_rate: 1
#        optimize_internal_loss: True
#      mf_factors: 64
#      dropout: 0
#      is_mf_train: True
#      is_mlp_train: True
#      batch_size: 256
#      epochs: 100
#      lr: 0.001
#      m: 4
#    ItemKNN: #from TOIS
#      meta:
#        save_recs: True
#        verbose: True
#        hyper_max_evals: 20
#        hyper_opt_alg: tpe
#      neighbors: [uniform, 5, 1000]
#      similarity: [cosine, jaccard, dice, mahalanobis, euclidean]
#    UserKNN: #from TOIS
#      meta:
#        hyper_max_evals: 20
#        hyper_opt_alg: tpe
#        save_recs: True
#        verbose: True
#      neighbors: [ uniform, 5, 1000 ]
#      similarity: [cosine, jaccard, dice, mahalanobis, euclidean]
#    MultiVAE: # from original paper
#      meta:
#        hyper_max_evals: 20
#        hyper_opt_alg: tpe
#        save_recs: True
#        verbose: True
#        # optimize_internal_loss: True
#      lr: [loguniform, -11.512925464970229, 0] # exploration taken from TOIS
#      epochs: 200
#      batch_size: [ 128, 256, 512 ]
#      intermediate_dim: 600
#      latent_dim: 200
#      dropout_pkeep: 0.5
#      reg_lambda: [loguniform, -11.512925464970229, 0] # exploration taken from TOIS
    Slim: #from TOIS
      meta:
        hyper_max_evals: 1
        hyper_opt_alg: tpe
        verbose: True
        save_recs: True
      l1_ratio: 1.15e-04
      alpha: 0.0526
      neighborhood: 1000
    external.iALS:
      meta:
        verbose: True
        save_recs: True
      factors: 30
      alpha: 50
      epsilon: 0.0010
      reg: 1.00e-02
      scaling: linear
      epochs: 45
#    external.EASER:
#      meta:
#        verbose: True
#        save_recs: True
#        hyper_max_evals: 1
#        hyper_opt_alg: tpe
#      l2_norm: 3540
#    external.RP3beta: #from TOIS
#      meta:
#        hyper_max_evals: 1
#        hyper_opt_alg: tpe
#        verbose: True
#        save_recs: True
#      neighborhood: 1000
#      alpha: 0.8616
#      beta: 0.4255
#      normalize_similarity: True
#    PureSVD:
#      meta:
#        hyper_max_evals: 1
#        hyper_opt_alg: tpe
#        verbose: True
#        save_recs: True
#      factors: 50
#      seed: 42
